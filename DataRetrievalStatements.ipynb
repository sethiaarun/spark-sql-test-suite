{"cells":[{"cell_type":"markdown","source":["## Test Cases for Data Retrieval Statements used by Apache Spark\n","**Spark SQL 3.3 Data Retrieval Statements Reference** \n","https://spark.apache.org/docs/3.3.0/sql-ref-syntax.html#data-retrieval-statements\n","\n","To store these results configure data <mark>**storage account and container**</mark>."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"31b4a0ed-dc8f-4e28-a906-448936371cc8"},{"cell_type":"code","source":["!pip install unittest-xml-reporting xmltodict"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"84095056-4467-44ac-a636-6030ca9f3e57"},{"cell_type":"markdown","source":["## Configure Result Storage Location"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"dac28517-24d2-4bc5-a073-3a27a1707bb1"},{"cell_type":"code","source":["storage_account=\"\"\n","result_container=\"\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"16c2aede-6051-4bed-bbb7-054e894ac2de"},{"cell_type":"markdown","source":["## Initialize Common Variables for the test run"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1f170c66-dbe2-4738-b396-ce2c7f470bea"},{"cell_type":"code","source":["import time\n","\n","# Don't change these variables\n","TEST_SUITE= \"SPARK_SQL_DML\"\n","RESULT_FILE_NAME=\"data_retrieval_test_result.parquet\"\n","RAW_RESULT_FILE_NAME=\"raw_data_retrieval_test_result.parquet\"\n","# Test Run ID\n","TEST_RUN_ID= round(time.time()*1000)\n","# Test platform\n","PLATFORM = \"nameoftheplatform\"\n","# Prefix for all tables\n","PREFIX = PLATFORM\n","SUFFIX = TEST_RUN_ID\n","# Spark SQL function\n","sql=spark.sql"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9253f197-5e4d-41bb-b38d-ce18739ecf0e"},{"cell_type":"markdown","source":["### Set Common Spark Configurations"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"51c090ab-3455-41bb-b267-688b01511007"},{"cell_type":"code","source":["sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5f8f0f5b-6d75-42df-be1f-f38f37bcbf4a"},{"cell_type":"markdown","source":["## DML - Insert Table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"06a69bcf-b013-4dc7-bb2c-be4420ed1a98"},{"cell_type":"code","source":["import unittest\n","import numpy as np\n","\n","class DataRetrievalInsertTableTest(unittest.TestCase):\n","    \"\"\"Test cases for data retrieval\"\"\"\n","\n","    customer_table_name=f\"{PREFIX}_customer_insert_table_{SUFFIX}\"\n","    tx_table_name=f\"{PREFIX}_transaction_insert_table_{SUFFIX}\"\n","    view_name = f\"{PREFIX}_testtx_customer_{SUFFIX}\"\n","\n","    \n","    @classmethod\n","    def setUpClass(cls):\n","        customer_table_name_sql = f\"CREATE TABLE {cls.customer_table_name} (cust_id INT, name STRING, age INT) \\\n","                        PARTITIONED BY (state STRING)\"\n","        tx_table_name_sql = f\"CREATE TABLE {cls.tx_table_name} (id INT, cust_id INT, tx_date STRING, total_amount Float) \\\n","                        PARTITIONED BY (year INT)\"\n","        try:\n","            sql(customer_table_name_sql)\n","            sql(tx_table_name_sql)\n","            sql(f\"INSERT INTO {cls.customer_table_name} VALUES \\\n","                 (1,'abcd',20,'TX'),(2,'bbcd',18,'CA'),(3,'c',25,'TX'),(4,'d',20,'WA'),(5,'e',18,'AK'),(6,'f_cd',NULL,'CA');\")\n","            sql(f\"INSERT INTO {cls.tx_table_name} VALUES \\\n","                 (1,1,'2019-01-01',40.56,2019),(2,2,'2022-11-01',10.56,2022),(3,1,'2023-12-05',10.34,2023),(4,3,'2021-11-05',8.00,2021),(5,6,'2019-10-12',12.45,2019),(6,2,'2023-10-12',15.45,2023);\")\n","            \n","            sql(f\"CREATE OR REPLACE VIEW {cls.view_name} AS SELECT A.*,B.name, b.state FROM {cls.tx_table_name} A \\\n","                                INNER JOIN {cls.customer_table_name} B ON A.cust_id=B.cust_id\")\n","        except Exception as ex:\n","            msg={'command':'InsertTable Setup failed','status':'fail'}\n","            cls.fail(f\"{msg}\")\n","    \n","\n","    def test_dataretrieval_001_cte_join_query(self):\n","        \"\"\"Data retrieval using CTE and INNER JOIN\"\"\"\n","        sql_cmd = f\"WITH t(custid, custname) AS (SELECT cust_id, name FROM {self.customer_table_name} WHERE state='TX') \\\n","                        SELECT custname,total_amount,year FROM {self.tx_table_name} INNER JOIN t ON t.custid= {self.tx_table_name}.cust_id;\"\n","        try:\n","            df=sql(sql_cmd)\n","            self.assertEqual(df.count(),3)\n","            self.assertEqual(len(df.columns),3)\n","        except Exception as ex:\n","            msg={'command':'INNER JOIN CTE (Common Table Expression)','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_002_cte_view_query(self):\n","        \"\"\"Data retrieval using CTE with View statement\"\"\"\n","        view_name = f\"{PREFIX}_cte_view_{SUFFIX}\"\n","        sql_cmd = f\"CREATE VIEW {view_name} AS WITH t(custid, custname) AS (SELECT cust_id, name FROM {self.customer_table_name} WHERE state='TX') \\\n","                        SELECT custid,custname FROM t\"\n","        try:\n","            sql(sql_cmd)\n","            df=sql(f\"SELECT * FROM {view_name}\")\n","            self.assertEqual(df.count(),2)\n","            self.assertEqual(len(df.columns),2)\n","        except Exception as ex:\n","            msg={'command':'Create View CTE (Common Table Expression)','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","        finally:\n","            sql(f\"DROP VIEW IF EXISTS {view_name}\")\n","\n","    def test_dataretrieval_003_cluster_by_query(self):\n","        \"\"\"Data retrieval using CLUSTER BY statement\"\"\"\n","        sql_cmd_without_cluster = f\"SELECT * FROM {self.customer_table_name}\"\n","        sql_cmd_with_cluster = f\"SELECT * FROM {self.customer_table_name} CLUSTER BY cust_id\"\n","        current_partition_conf =  spark.conf.get(\"spark.sql.shuffle.partitions\",'200')\n","        try:\n","            # without cluster\n","            sql(\"SET spark.sql.shuffle.partitions = 2;\")\n","            df_without_cluster=sql(sql_cmd_without_cluster)\n","            columns_without_cluster=df_without_cluster.toPandas()\n","            customerIds_without_cluster=list(columns_without_cluster['cust_id'])\n","            # with cluster The rows are sorted based on cust_id within each partition\n","            df_with_cluster=sql(sql_cmd_with_cluster)\n","            columns_with_cluster=df_with_cluster.toPandas()\n","            customerIds_with_cluster=list(columns_with_cluster['cust_id'])\n","            self.assertNotEqual(customerIds_with_cluster,customerIds_without_cluster)\n","            self.assertEqual(customerIds_with_cluster,[1,2,3,4,5,6])\n","        except Exception as ex:\n","            msg={'command':'SELECT with Cluster By','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","        finally:\n","            # rollback configuration\n","            spark.conf.set(\"spark.sql.shuffle.partitions\",current_partition_conf)\n","\n","    def test_dataretrieval_004_group_filter_by_query(self):\n","        \"\"\"Data retrieval using GROUP BY FILTER WHERE statement\"\"\"\n","        sql_cmd = f\"SELECT cust_id,sum(total_amount) FILTER (\\\n","                WHERE year IN (2019,2021) \\\n","              ) AS `sum(amount)` FROM {self.tx_table_name} GROUP by cust_id ORDER BY cust_id\"\n","        try:\n","            df=sql(sql_cmd)\n","            self.assertGreater(df.count(),1)\n","        except Exception as ex:\n","            msg={'command':'GROUP BY FILTER','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_005_first_last_query(self):\n","        \"\"\"Data retrieval using FIRST AND LAST\"\"\"\n","        sql_cmd = f\"SELECT FIRST(id) as firstid, LAST(id) as lastId FROM {self.tx_table_name}\"\n","        try:\n","            df=sql(sql_cmd)\n","            self.assertEqual(df.count(),1)\n","            self.assertEqual(len(df.columns),2)\n","        except Exception as ex:\n","            msg={'command':'GROUP BY FILTER','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_006_group_having_query(self):\n","        \"\"\"Data retrieval using GROUP BY Having\"\"\"\n","        sql_cmd = f\"SELECT year,sum(total_amount) as sumtotal  FROM {self.tx_table_name} GROUP by Year Having Year>2019 order by year\"\n","        try:\n","            df=sql(sql_cmd)\n","            pandas_df = df.toPandas()\n","            self.assertEqual(list(pandas_df['year']),[2021,2022,2023])\n","            self.assertEqual(df.count(),3)\n","        except Exception as ex:\n","            msg={'command':'GROUP BY Having Order By','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_007_inline_table(self):\n","        \"\"\"Data retrieval Inline Table\"\"\"\n","        sql_cmd = f\"SELECT country, {self.customer_table_name}.state,name FROM {self.customer_table_name} \\\n","                  INNER JOIN VALUES ('USA', 'TX'),('USA', 'CA'), ('USA','WA'), ('USA','AK') as countrystate(country, state) ON countrystate.state={self.customer_table_name}.state\"\n","        try:\n","            df=sql(sql_cmd)\n","            self.assertEqual(len(df.columns),3)\n","        except Exception as ex:\n","            msg={'command':'Inline Table Join','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_008_file_query(self):\n","        \"\"\"Data retrieval From file query\"\"\"\n","        file_name = f\"Files/test_dataretrieval_008_file_query/{PREFIX}_parquet_query_read_{SUFFIX}.parquet\"\n","        sql_cmd = f\"SELECT * FROM parquet.`{file_name}`\"\n","        try:\n","            df = sql(f\"SELECT * FROM {self.customer_table_name}\")\n","            df.write.parquet(file_name)\n","            read_file_df=sql(sql_cmd)\n","            self.assertEqual(df.count(), read_file_df.count())\n","        except Exception as ex:\n","            msg={'command':'Query a file with a parquet format ','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_009_like_predicate1(self):\n","        \"\"\"Data retrieval Like predicate filter\"\"\"\n","        try:\n","            df = sql(f\"SELECT * FROM {self.customer_table_name} WHERE state like 'T%'\")\n","            self.assertEqual(df.count(), 2)\n","        except Exception as ex:\n","            msg={'command':'Like predicate filter search pattern %','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_009_like_predicate2(self):\n","        \"\"\"Data retrieval Like predicate filter\"\"\"\n","        try:\n","            df = sql(f\"SELECT * FROM {self.customer_table_name} WHERE name like '_bcd' \")\n","            self.assertEqual(df.count(), 2)\n","        except Exception as ex:\n","            msg={'command':'Like predicate filter search pattern underscore','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_009_like_predicate3(self):\n","        \"\"\"Data retrieval Like predicate filter\"\"\"\n","        try:\n","            df = sql(f\"SELECT * FROM {self.customer_table_name} WHERE name like '%\\\\_%' \")\n","            self.assertEqual(df.count(), 1)\n","        except Exception as ex:\n","            msg={'command':'Like predicate filter esc_char','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_010_tablesample(self):\n","        \"\"\"Data retrieval Table Sample\"\"\"\n","        try:\n","            df = sql(f\"SELECT * FROM {self.customer_table_name} TABLESAMPLE (50 PERCENT)\")\n","            row_count=df.count()\n","            self.assertGreater(row_count, 0)\n","            df = sql(f\"SELECT * FROM {self.customer_table_name} TABLESAMPLE (5 ROWS)\")\n","            row_count=df.count()\n","            self.assertEqual(row_count, 5)\n","            df = sql(f\"SELECT * FROM {self.customer_table_name} TABLESAMPLE (BUCKET 2 OUT OF 6)\")\n","            row_count=df.count()\n","            self.assertGreater(row_count, 0)\n","        except Exception as ex:\n","            msg={'command':'TABLESAMPLE query','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_011_orderby_null_first_last(self):\n","        \"\"\"Data retrieval Order By NULL FIRST and NULL LAST\"\"\"\n","        try:\n","            df_null_first = sql(f\"SELECT * FROM {self.customer_table_name} ORDER BY age DESC NULLS FIRST \")\n","            df_null_first_p=df_null_first.toPandas()\n","            age_null_first=list(df_null_first_p['age'])\n","            self.assertTrue(np.isnan(age_null_first[0]))\n","            df_null_last = sql(f\"SELECT * FROM {self.customer_table_name} ORDER BY age DESC NULLS LAST \")\n","            df_null_last_p=df_null_last.toPandas()\n","            age_null_last=list(df_null_last_p['age'])\n","            self.assertTrue(np.isnan(age_null_last[len(age_null_last)-1]))\n","        except Exception as ex:\n","            msg={'command':'Order By NULL FIRST and NULL LAST','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_012_limit(self):\n","        \"\"\"Data retrieval LIMIT ALL\"\"\"\n","        try:\n","            df_limit_2 = sql(f\"SELECT * FROM {self.customer_table_name} LIMIT 2\")\n","            self.assertEqual(df_limit_2.count(),2)\n","            df_limit_all = sql(f\"SELECT * FROM {self.customer_table_name} LIMIT ALL\")\n","            self.assertEqual(df_limit_all.count(),6)\n","        except Exception as ex:\n","            msg={'command':'LIMIT and LIMIT ALL','status':'fail'}\n","            self.fail(f\"{msg}\")\n"," \n","    def test_dataretrieval_012_limit(self):\n","        \"\"\"Data retrieval LIMIT ALL\"\"\"\n","        try:\n","            df_limit_2 = sql(f\"SELECT * FROM {self.customer_table_name} LIMIT 2\")\n","            self.assertEqual(df_limit_2.count(),2)\n","            df_limit_all = sql(f\"SELECT * FROM {self.customer_table_name} LIMIT ALL\")\n","            self.assertEqual(df_limit_all.count(),6)\n","        except Exception as ex:\n","            msg={'command':'LIMIT and LIMIT ALL','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_013_window_dense_rank(self):\n","        \"\"\"Window functions - DENSE_RANK\"\"\"\n","        try:\n","            # Test dense rank\n","            df_dense_rank = sql(f\"SELECT name, state, total_amount, DENSE_RANK() OVER (PARTITION BY state ORDER BY total_amount \\\n","                                        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank FROM {self.view_name}\")\n","            df_dense_rank_p=df_dense_rank.toPandas()\n","            self.assertEqual(list(df_dense_rank_p['state']),['CA','CA','CA','TX','TX','TX'])\n","            self.assertEqual(list(df_dense_rank_p['dense_rank']),[1,2,3,1,2,3])\n","        except Exception as ex:\n","            msg={'command':'Windows DENSE_RANK','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_014_window_cum_dist(self):\n","        \"\"\"Window functions - CUME_DIST\"\"\"\n","        try:\n","            df = sql(f\"SELECT name, state, total_amount, CUME_DIST() OVER (PARTITION BY state ORDER BY total_amount \\\n","                                        RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cume_dist FROM {self.view_name}\")\n","            df_p=df.toPandas()\n","            self.assertEqual(sum(list(df_p['cume_dist'])),4)\n","            self.assertEqual(len(df_p),6)\n","        except Exception as ex:\n","            msg={'command':'Windows CUME_DIST','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_014_window_min_over(self):\n","        \"\"\"Window functions - MIN OVER\"\"\"\n","        try:\n","            df = sql(f\"SELECT name, state, total_amount, Min(total_amount) OVER (PARTITION BY state ORDER BY total_amount) AS min FROM {self.view_name}\")\n","            df_p=df.toPandas()\n","            l_min = list(df_p['min'])\n","            self.assertEqual(list(map(lambda x:round(x,2), l_min)),[10.56,10.56,10.56,8,8,8])\n","        except Exception as ex:\n","            msg={'command':'Windows MIN OVER','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_015_window_lag_lead(self):\n","        \"\"\"Window functions - LAG AND LEAD\"\"\"\n","        try:\n","            df = sql(f\"SELECT name, state, total_amount, LAG(total_amount) OVER (PARTITION BY state ORDER BY total_amount) AS lag,\\\n","                                   LEAD(total_amount,1,0) OVER (PARTITION BY state ORDER BY total_amount) as lead  FROM {self.view_name}\")\n","            df_p=df.toPandas()\n","            self.assertTrue(np.isnan(list(df_p['lag'])[0]))\n","            self.assertEqual(list(df_p['lead'])[len(df_p)-1],0.0)\n","        except Exception as ex:\n","            msg={'command':'Windows LAG LEAD','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dataretrieval_016_window_lateral_view(self):\n","        \"\"\"LATERAL VIEW\"\"\"\n","        try:\n","            df = sql(f\"SELECT * FROM {self.customer_table_name} LATERAL VIEW EXPLODE(ARRAY('Y', 'N')) tableName AS test_flag\")\n","            # total 6 rows multiply by 2\n","            self.assertEqual(df.count(),12)\n","            df = sql(f\"SELECT * FROM {self.customer_table_name} LATERAL VIEW EXPLODE(ARRAY()) AS empty_col\")\n","            self.assertEqual(df.count(),0)\n","            df = sql(f\"SELECT * FROM {self.customer_table_name} LATERAL VIEW OUTER EXPLODE(ARRAY()) AS empty_col2\")\n","            self.assertEqual(df.count(),6)\n","        except Exception as ex:\n","            msg={'command':'LATERAL VIEW ','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    @classmethod\n","    def tearDownClass(cls):\n","        \"\"\"tear down\"\"\"\n","        sql(f\"DROP TABLE IF EXISTS {cls.customer_table_name}\")\n","        sql(f\"DROP TABLE IF EXISTS {cls.tx_table_name}\")\n","        sql(f\"DROP VIEW IF EXISTS {cls.view_name}\")\n","\n","#TODO: DISTRIBUTE BY, Hints, PIVOT "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"31f8bd52-55be-4e19-907f-c8bc80b982af"},{"cell_type":"markdown","source":["### Execute Test Case"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fa8e605d-7c55-47aa-802a-b8d2f60b500f"},{"cell_type":"code","source":["import io\n","import xmlrunner\n","loader = unittest.TestLoader()\n","suite  = unittest.TestSuite()\n","\n","# add tests to the test suite\n","suite.addTests(loader.loadTestsFromTestCase(DataRetrievalInsertTableTest))\n","\n","# initialize a runner, pass it your suite and run it\n","out = io.BytesIO()\n","runner = xmlrunner.XMLTestRunner(output=out)\n","result = runner.run(suite)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"92d0b3bf-2cc8-4f59-b37d-3aab8b1effdc"},{"cell_type":"markdown","source":["## Report for Test"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3935d6b2-8879-4bcb-b25a-dfc100be48c1"},{"cell_type":"code","source":["from pyspark.sql.functions import col, explode,isnull,from_json, expr, to_json, coalesce, lit\n","from pyspark.sql.types import StructType,StructField,StringType\n","import json\n","import xmltodict\n","\n","dict_result=xmltodict.parse(out.getvalue())\n","json_result = json.loads(json.dumps(dict_result,indent=4).replace('@',''))\n","test_suites=json_result['testsuites']['testsuite']\n","\n","df = spark.read.json(sc.parallelize([test_suites]))\n","fail_schema = StructType([\n","  StructField(\"command\", StringType(), True),\n","  StructField(\"status\", StringType(),  True)\n","])\n","\n","test_cases_df= df.withColumn('ts',explode('testcase')).drop(col('testcase'))\n","\n","if \"failure:\" in test_cases_df.schema.simpleString():\n"," explode_df= test_cases_df.withColumn('fail',from_json(col('ts.failure.message'),fail_schema)).drop(col('ts.failure'))\n","else:\n"," explode_df= test_cases_df.withColumn(\"fail\",from_json(expr(\"to_json(named_struct('command', '', 'status', 'pass'))\"),fail_schema))\n"," \n","df_test_result=explode_df.select(col(\"errors\").alias(\"errorInSuite\"),col(\"failures\").alias(\"failedInSuite\"),col(\"name\").alias(\"suitename\"),\\\n","      \"skipped\",col(\"tests\").alias(\"totalTest\"), col(\"timestamp\").alias(\"executionTime\"),col(\"ts.name\").alias(\"testCaseName\"), \\\n","       col(\"ts.time\").alias(\"testCaseTime\"),coalesce(col(\"fail.command\"), lit(\"\")).alias(\"failcommand\"),coalesce(col(\"fail.status\"), lit(\"pass\")).alias(\"status\"))\n","\n","if (len(storage_account)>0 and len(result_container)>0):\n","    # save result to storage\n","    storage_path = f\"abfs://{result_container}@{storage_account}.dfs.core.windows.net/{TEST_RUN_ID}/{PLATFORM}/{TEST_SUITE}\"\n","    # write raw results\n","    df.write.parquet(f\"{storage_path}/{RAW_RESULT_FILE_NAME}\")\n","    # write transformed results\n","    df_test_result.write.parquet(f\"{storage_path}/{RESULT_FILE_NAME}\") \n","else:\n","    print(\"configure storage path to store results\")\n","    df_test_result.show(200,False)    "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0d350743-2897-493f-92f1-5bb0c714972c"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"host":{"synapse_widget":{"token":"1b31f066-c832-452c-a512-517daaab6a42","state":{"d198bae5-6171-4f59-a45e-241c0e2d26ae":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"bbcd","1":"CA","2":"10.56","3":"10.56","index":1},{"0":"f_cd","1":"CA","2":"12.45","3":"10.56","index":2},{"0":"bbcd","1":"CA","2":"15.45","3":"10.56","index":3},{"0":"c","1":"TX","2":"8.0","3":"8.0","index":4},{"0":"abcd","1":"TX","2":"10.34","3":"8.0","index":5},{"0":"abcd","1":"TX","2":"40.56","3":"8.0","index":6}],"schema":[{"key":"0","name":"name","type":"string"},{"key":"1","name":"state","type":"string"},{"key":"2","name":"total_amount","type":"float"},{"key":"3","name":"min","type":"float"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["2"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{"d198bae5-6171-4f59-a45e-241c0e2d26ae":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"bbcd","1":"CA","2":"10.56","3":"10.56","index":1},{"0":"f_cd","1":"CA","2":"12.45","3":"10.56","index":2},{"0":"bbcd","1":"CA","2":"15.45","3":"10.56","index":3},{"0":"c","1":"TX","2":"8.0","3":"8.0","index":4},{"0":"abcd","1":"TX","2":"10.34","3":"8.0","index":5},{"0":"abcd","1":"TX","2":"40.56","3":"8.0","index":6}],"schema":[{"key":"0","name":"name","type":"string"},{"key":"1","name":"state","type":"string"},{"key":"2","name":"total_amount","type":"float"},{"key":"3","name":"min","type":"float"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["2"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"307fda43-cb06-4c49-bb3f-a6028617e553"}],"default_lakehouse":"307fda43-cb06-4c49-bb3f-a6028617e553","default_lakehouse_name":"unitestcases_examples","default_lakehouse_workspace_id":"4f212d69-90d6-4648-b0c3-70dbd6d9a307"}}},"nbformat":4,"nbformat_minor":5}