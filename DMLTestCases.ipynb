{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Test Cases for DML used by Apache Spark\n",
        "**Spark SQL 3.3 DML Reference** \n",
        "https://spark.apache.org/docs/3.3.0/sql-ref-syntax.html#dml-statements\n",
        "\n",
        "To store these results configure data <mark>**storage account and container**</mark>.\n",
        "\n",
        "Configure DIRECTORY_PATH for Insert Overwrite "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unittest-xml-reporting xmltodict"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Result Storage Location"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "storage_account=\"\"\n",
        "result_container=\"\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "Spark33Pool",
              "session_id": "16",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-10-24T20:01:38.3500823Z",
              "session_start_time": null,
              "execution_start_time": "2023-10-24T20:04:13.8348815Z",
              "execution_finish_time": "2023-10-24T20:04:14.0936495Z",
              "spark_jobs": null,
              "parent_msg_id": "20160b2b-9c27-4085-b34f-c0a004f0cf81"
            },
            "text/plain": "StatementMeta(Spark33Pool, 16, 3, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Common Variables for the test run"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Don't change these variables\n",
        "TEST_SUITE= \"SPARK_SQL_DML\"\n",
        "RESULT_FILE_NAME=\"dml_test_result.parquet\"\n",
        "RAW_RESULT_FILE_NAME=\"raw_dml_test_result.parquet\"\n",
        "# Test Run ID\n",
        "TEST_RUN_ID= round(time.time()*1000)\n",
        "# Test platform\n",
        "PLATFORM = \"nameoftheplatform\"\n",
        "# Prefix for all tables\n",
        "PREFIX = PLATFORM\n",
        "SUFFIX = TEST_RUN_ID\n",
        "# Spark SQL function\n",
        "sql=spark.sql"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Directory Path for Insert Overwrite "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIRECTORY_PATH=\"Files/DML/InsertOverWriteTest\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Common Spark Configurations"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DML - Insert Table"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class DMLInsertTableTest(unittest.TestCase):\n",
        "\n",
        "    table_name=f\"{PREFIX}_student_insert_table_{SUFFIX}\"\n",
        "    \n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        table_name_sql = f\"CREATE TABLE {cls.table_name} (id INT, name STRING) \\\n",
        "                        PARTITIONED BY (age INT)\"\n",
        "        try:\n",
        "            sql(table_name_sql)\n",
        "        except Exception as ex:\n",
        "            msg={'command':'InsertTable Setup failed','status':'fail'}\n",
        "            cls.fail(f\"{msg}\")\n",
        "\n",
        "    def test_dml_insert_001_values(self):\n",
        "        \"\"\"insert values to a table\"\"\"\n",
        "        try:\n",
        "            sql(f\"INSERT INTO {self.table_name} VALUES \\\n",
        "                 (1,'a',10),(2,'b',20),(3,'c',30);\")\n",
        "            record_count=sql(f\"SELECT * FROM {self.table_name}\").count()\n",
        "            self.assertEqual(record_count,3)\n",
        "        except Exception as ex:\n",
        "            msg={'command':'INSERT USING VALUES','status':'fail'}\n",
        "            self.fail(f\"{msg}\")\n",
        "\n",
        "    def test_dml_insert_002_using_select(self):\n",
        "        \"\"\"insert values using select\"\"\"\n",
        "        person_table_name=f\"{PREFIX}_person_insert_table_{SUFFIX}\"\n",
        "        try:\n",
        "            person_table_name_sql = f\"CREATE TABLE {person_table_name} (id INT, name STRING, age INT) \\\n",
        "                        PARTITIONED BY (student_id INT)\"\n",
        "            sql(person_table_name_sql)\n",
        "            sql(f\"INSERT INTO {person_table_name} PARTITION (student_id=1234) \\\n",
        "                        SELECT id, name,age FROM {self.table_name} WHERE id=1\")\n",
        "            record_count=sql(f\"SELECT * FROM {person_table_name}\").count()\n",
        "            self.assertEqual(record_count,1)\n",
        "        except Exception as ex:\n",
        "            msg={'command':'INSERT INTO PARTITION SELECT','status':'fail'}\n",
        "            self.fail(f\"{msg}\")\n",
        "\n",
        "        finally:\n",
        "            sql(f\"DROP TABLE IF EXISTS {person_table_name}\")\n",
        "\n",
        "    def test_dml_insert_003_using_table(self):\n",
        "        \"\"\"insert using table\"\"\"\n",
        "        table_name_copy=f\"{PREFIX}_student_insert_table_copy_{SUFFIX}\"\n",
        "        try:\n",
        "            table_name_copy_sql = f\"CREATE TABLE {table_name_copy} (id INT, name STRING) \\\n",
        "                        PARTITIONED BY (age INT)\"\n",
        "            sql(table_name_copy_sql)\n",
        "            sql(f\"INSERT INTO {table_name_copy} TABLE {self.table_name}\")\n",
        "            record_count=sql(f\"SELECT * FROM {table_name_copy}\").count()\n",
        "            self.assertEqual(record_count,3)\n",
        "            sql(f\"DROP TABLE IF EXISTS {table_name_copy}\")\n",
        "        except Exception as ex:\n",
        "            msg={'command':'INSERT INTO TABLE','status':'fail'}\n",
        "            self.fail(f\"{msg}\")\n",
        "\n",
        "        finally:\n",
        "            sql(f\"DROP TABLE IF EXISTS {table_name_copy}\")\n",
        "\n",
        "    def test_dml_insert_004_using_from(self):\n",
        "        \"\"\"insert using from\"\"\"\n",
        "        table_name_cp_from=f\"{PREFIX}_student_insert_table_cp_from_{SUFFIX}\"\n",
        "        try:\n",
        "            table_name_copy_sql = f\"CREATE TABLE {table_name_cp_from} (id INT, name STRING) \\\n",
        "                        PARTITIONED BY (age INT)\"\n",
        "            sql(table_name_copy_sql)\n",
        "            sql(f\"INSERT INTO {table_name_cp_from} FROM {self.table_name} SELECT id, name,age WHERE id=1\")\n",
        "            record_count=sql(f\"SELECT * FROM {table_name_cp_from}\").count()\n",
        "            self.assertEqual(record_count,1)\n",
        "        except Exception as ex:\n",
        "            msg={'command':'INSERT INTO FROM SELECT','status':'fail'}\n",
        "            self.fail(f\"{msg}\")\n",
        "        \n",
        "        finally:\n",
        "            sql(f\"DROP TABLE IF EXISTS {table_name_cp_from}\")\n",
        "    \n",
        "    def test_dml_insert_005_with_column_list_with_part_spec(self):\n",
        "        \"\"\"insert with both a partition spec and a column list\"\"\"\n",
        "        table_name_col_part=f\"{PREFIX}_student_insert_table_col_list_{SUFFIX}\"\n",
        "        try:\n",
        "            table_name_col_part_sql = f\"CREATE TABLE {table_name_col_part} (id INT, name STRING) \\\n",
        "                        PARTITIONED BY (age INT)\"\n",
        "            sql(table_name_col_part_sql)\n",
        "            sql(f\"INSERT INTO {table_name_col_part} PARTITION (age=20) (id,name) VALUES (1,'a'),(2,'b')\")\n",
        "            record_count=sql(f\"SELECT * FROM {table_name_col_part}\").count()\n",
        "            self.assertEqual(record_count,2)\n",
        "        except Exception as ex:\n",
        "            msg={'command':'INSERT With Column List and Partition Spec','status':'fail'}\n",
        "            self.fail(f\"{msg}\")\n",
        "        \n",
        "        finally:\n",
        "            sql(f\"DROP TABLE IF EXISTS {table_name_col_part}\")\n",
        "        \n",
        "    \n",
        "    def test_dml_insert_006_with_typed_date_part_col(self):\n",
        "        \"\"\"insert Using a Typed Date Literal for a Partition Column Value\"\"\"\n",
        "        table_name_typed_date_part=f\"{PREFIX}_student_insert_table_date_part_{SUFFIX}\"\n",
        "        try:\n",
        "            table_name_copy_sql = f\"CREATE TABLE {table_name_typed_date_part} (id INT, name STRING) \\\n",
        "                        PARTITIONED BY (birthday DATE)\"\n",
        "            sql(table_name_copy_sql)\n",
        "            sql(f\"INSERT INTO {table_name_typed_date_part} PARTITION (birthday = date'2019-01-02') (id,name) VALUES (1,'a'),(2,'b')\")\n",
        "            record_count=sql(f\"SELECT * FROM {table_name_typed_date_part}\").count()\n",
        "            self.assertEqual(record_count,2)\n",
        "        except Exception as ex:\n",
        "            msg={'command':'INSERT Using a Typed Date Literal for a Partition Column Value','status':'fail'}\n",
        "            self.fail(f\"{msg}\")\n",
        "        \n",
        "        finally:\n",
        "            sql(f\"DROP TABLE IF EXISTS {table_name_typed_date_part}\")\n",
        "    \n",
        "    def test_dml_insert_007_overwrite_spark_format(self):\n",
        "        \"\"\"insert overwrite for Spark format\"\"\"\n",
        "        output_path = f\"{DIRECTORY_PATH}/{TEST_RUN_ID}/spark\"\n",
        "        try:\n",
        "            df = sql(f\"SELECT * FROM {self.table_name}\")\n",
        "            df.write.parquet(f'{output_path}')\n",
        "            sql_cmd = f\"INSERT OVERWRITE DIRECTORY \\\n",
        "                    USING parquet \\\n",
        "                    OPTIONS ('path' '{output_path}') \\\n",
        "                  SELECT * FROM {self.table_name};\"\n",
        "            sql(sql_cmd)\n",
        "        except Exception as ex:\n",
        "            msg={'command':'INSERT OVERWRITE DIRECTORY USING parquet','status':'fail'}\n",
        "            self.fail(f\"{msg}\")\n",
        "        \n",
        "    def test_dml_insert_008_overwrite_hive_format(self):\n",
        "        \"\"\"insert overwrite for hive format\"\"\"\n",
        "        output_path = f\"{DIRECTORY_PATH}/{TEST_RUN_ID}/hive\"\n",
        "        try:\n",
        "            df = sql(f\"SELECT * FROM {self.table_name}\")\n",
        "            df.write.parquet(f'{output_path}')\n",
        "            sql_cmd = f\"INSERT OVERWRITE LOCAL DIRECTORY '{output_path}'\\\n",
        "                    STORED AS orc \\\n",
        "                  SELECT * FROM {self.table_name};\"\n",
        "            sql(sql_cmd)\n",
        "        except Exception as ex:\n",
        "            msg={'command':'INSERT OVERWRITE LOCAL DIRECTORY STORED AS orc','status':'fail'}\n",
        "            self.fail(f\"{msg}\")\n",
        "\n",
        "    def test_dml_insert_009_load_into_table(self):\n",
        "        \"\"\"insert LOAD INTO TABLE\"\"\"\n",
        "        input_path = f\"{DIRECTORY_PATH}/{TEST_RUN_ID}/hive\"\n",
        "        table_name_load=f\"{PREFIX}_student_insert_table_load_{SUFFIX}\"\n",
        "        table_sql = f\"CREATE TABLE {table_name_load} (id INT, name STRING, age INT) USING Hive\"\n",
        "        try:\n",
        "            sql(table_sql)\n",
        "            sql_cmd = f\"LOAD DATA LOCAL INPATH '{input_path}' OVERWRITE INTO TABLE {table_name_load}\"\n",
        "            sql(sql_cmd)\n",
        "            df = sql(f\"SELECT * FROM {table_name_load}\").count()\n",
        "            self.assertEqual(record_count,2)\n",
        "        except Exception as ex:\n",
        "            msg={'command':'LOAD DATA LOCAL INTO TABLE','status':'fail'}\n",
        "            self.fail(f\"{msg}\")\n",
        "\n",
        "        finally:\n",
        "            sql(f\"DROP TABLE IF EXISTS {table_name_load}\")\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        \"\"\"tear down\"\"\"\n",
        "        sql(f\"DROP TABLE IF EXISTS {cls.table_name}\")\n",
        "\n",
        "\n",
        "# TODO: Add Hive complex datatype, avro and "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute Test Case"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import xmlrunner\n",
        "loader = unittest.TestLoader()\n",
        "suite  = unittest.TestSuite()\n",
        "\n",
        "# add tests to the test suite\n",
        "suite.addTests(loader.loadTestsFromTestCase(DMLInsertTableTest))\n",
        "\n",
        "\n",
        "# initialize a runner, pass it your suite and run it\n",
        "out = io.BytesIO()\n",
        "runner = xmlrunner.XMLTestRunner(output=out)\n",
        "result = runner.run(suite)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report for Test"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode,isnull,from_json, expr, to_json, coalesce, lit\n",
        "from pyspark.sql.types import StructType,StructField,StringType\n",
        "import json\n",
        "import xmltodict\n",
        "\n",
        "dict_result=xmltodict.parse(out.getvalue())\n",
        "json_result = json.loads(json.dumps(dict_result,indent=4).replace('@',''))\n",
        "test_suites=json_result['testsuites']['testsuite']\n",
        "\n",
        "df = spark.read.json(sc.parallelize([test_suites]))\n",
        "fail_schema = StructType([\n",
        "  StructField(\"command\", StringType(), True),\n",
        "  StructField(\"status\", StringType(),  True)\n",
        "])\n",
        "\n",
        "test_cases_df= df.withColumn('ts',explode('testcase')).drop(col('testcase'))\n",
        "\n",
        "if \"failure:\" in test_cases_df.schema.simpleString():\n",
        " explode_df= test_cases_df.withColumn('fail',from_json(col('ts.failure.message'),fail_schema)).drop(col('ts.failure'))\n",
        "else:\n",
        " explode_df= test_cases_df.withColumn(\"fail\",from_json(expr(\"to_json(named_struct('command', '', 'status', 'pass'))\"),fail_schema))\n",
        " \n",
        "df_test_result=explode_df.select(col(\"errors\").alias(\"errorInSuite\"),col(\"failures\").alias(\"failedInSuite\"),col(\"name\").alias(\"suitename\"),\\\n",
        "      \"skipped\",col(\"tests\").alias(\"totalTest\"), col(\"timestamp\").alias(\"executionTime\"),col(\"ts.name\").alias(\"testCaseName\"), \\\n",
        "       col(\"ts.time\").alias(\"testCaseTime\"),coalesce(col(\"fail.command\"), lit(\"\")).alias(\"failcommand\"),coalesce(col(\"fail.status\"), lit(\"pass\")).alias(\"status\"))\n",
        "\n",
        "df_test_result_summary=df_test_result.select(col('errorInSuite'),col('failedInSuite'),col('suitename'),col('skipped'),col('totaltest')).distinct()\n",
        "\n",
        "if (len(storage_account)>0 and len(result_container)>0):\n",
        "    # save result to storage\n",
        "    storage_path = f\"abfs://{result_container}@{storage_account}.dfs.core.windows.net/{TEST_RUN_ID}/{PLATFORM}/{TEST_SUITE}\"\n",
        "    # write raw results\n",
        "    df.write.parquet(f\"{storage_path}/{RAW_RESULT_FILE_NAME}\")\n",
        "    # write transformed results\n",
        "    df_test_result.write.parquet(f\"{storage_path}/{RESULT_FILE_NAME}\") \n",
        "    # write transformed results\n",
        "    df_test_result_summary.write.parquet(f\"{storage_path}/{RESULT_FILE_NAME}_summary\") \n",
        "else:\n",
        "    print(\"configure storage path to store results\")\n",
        "    df_test_result.show(200,False)    \n",
        "    df_test_result_summary.show(100,False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}