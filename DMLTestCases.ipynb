{"cells":[{"cell_type":"markdown","source":["## Test Cases for DML used by Apache Spark\n","**Spark SQL 3.3 DML Reference** \n","https://spark.apache.org/docs/3.3.0/sql-ref-syntax.html#dml-statements\n","\n","To store these results configure data <mark>**storage account and container**</mark>.\n","\n","Configure DIRECTORY_PATH for Insert Overwrite "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"31b4a0ed-dc8f-4e28-a906-448936371cc8"},{"cell_type":"code","source":["!pip install unittest-xml-reporting xmltodict"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"84095056-4467-44ac-a636-6030ca9f3e57"},{"cell_type":"markdown","source":["## Configure Result Storage Location"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"dac28517-24d2-4bc5-a073-3a27a1707bb1"},{"cell_type":"code","source":["storage_account=\"\"\n","result_container=\"\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"16c2aede-6051-4bed-bbb7-054e894ac2de"},{"cell_type":"markdown","source":["## Initialize Common Variables for the test run"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1f170c66-dbe2-4738-b396-ce2c7f470bea"},{"cell_type":"code","source":["import time\n","\n","# Don't change these variables\n","TEST_SUITE= \"SPARK_SQL_DML\"\n","RESULT_FILE_NAME=\"dml_test_result.parquet\"\n","RAW_RESULT_FILE_NAME=\"raw_dml_test_result.parquet\"\n","# Test Run ID\n","TEST_RUN_ID= round(time.time()*1000)\n","# Test platform\n","PLATFORM = \"nameoftheplatform\"\n","# Prefix for all tables\n","PREFIX = PLATFORM\n","SUFFIX = TEST_RUN_ID\n","# Spark SQL function\n","sql=spark.sql"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9253f197-5e4d-41bb-b38d-ce18739ecf0e"},{"cell_type":"markdown","source":["### Configure Directory Path for Insert Overwrite "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"14b009e5-2c44-41cf-931b-803dd06072f1"},{"cell_type":"code","source":["DIRECTORY_PATH=\"Files/DML/InsertOverWriteTest\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8a8568b3-a44f-40a1-a7d5-e638ca0a4583"},{"cell_type":"markdown","source":["### Set Common Spark Configurations"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"51c090ab-3455-41bb-b267-688b01511007"},{"cell_type":"code","source":["sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5f8f0f5b-6d75-42df-be1f-f38f37bcbf4a"},{"cell_type":"markdown","source":["## DML - Insert Table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"06a69bcf-b013-4dc7-bb2c-be4420ed1a98"},{"cell_type":"code","source":["import unittest\n","\n","class DMLInsertTableTest(unittest.TestCase):\n","\n","    table_name=f\"{PREFIX}_student_insert_table_{SUFFIX}\"\n","    \n","    @classmethod\n","    def setUpClass(cls):\n","        table_name_sql = f\"CREATE TABLE {cls.table_name} (id INT, name STRING) \\\n","                        PARTITIONED BY (age INT)\"\n","        try:\n","            sql(table_name_sql)\n","        except Exception as ex:\n","            msg={'command':'InsertTable Setup failed','status':'fail'}\n","            cls.fail(f\"{msg}\")\n","\n","    def test_dml_insert_001_values(self):\n","        \"\"\"insert values to a table\"\"\"\n","        try:\n","            sql(f\"INSERT INTO {self.table_name} VALUES \\\n","                 (1,'a',10),(2,'b',20),(3,'c',30);\")\n","            record_count=sql(f\"SELECT * FROM {self.table_name}\").count()\n","            self.assertEqual(record_count,3)\n","        except Exception as ex:\n","            msg={'command':'INSERT USING VALUES','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dml_insert_002_using_select(self):\n","        \"\"\"insert values using select\"\"\"\n","        person_table_name=f\"{PREFIX}_person_insert_table_{SUFFIX}\"\n","        try:\n","            person_table_name_sql = f\"CREATE TABLE {person_table_name} (id INT, name STRING, age INT) \\\n","                        PARTITIONED BY (student_id INT)\"\n","            sql(person_table_name_sql)\n","            sql(f\"INSERT INTO {person_table_name} PARTITION (student_id=1234) \\\n","                        SELECT id, name,age FROM {self.table_name} WHERE id=1\")\n","            record_count=sql(f\"SELECT * FROM {person_table_name}\").count()\n","            self.assertEqual(record_count,1)\n","        except Exception as ex:\n","            msg={'command':'INSERT INTO PARTITION SELECT','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","        finally:\n","            sql(f\"DROP TABLE IF EXISTS {person_table_name}\")\n","\n","    def test_dml_insert_003_using_table(self):\n","        \"\"\"insert using table\"\"\"\n","        table_name_copy=f\"{PREFIX}_student_insert_table_copy_{SUFFIX}\"\n","        try:\n","            table_name_copy_sql = f\"CREATE TABLE {table_name_copy} (id INT, name STRING) \\\n","                        PARTITIONED BY (age INT)\"\n","            sql(table_name_copy_sql)\n","            sql(f\"INSERT INTO {table_name_copy} TABLE {self.table_name}\")\n","            record_count=sql(f\"SELECT * FROM {table_name_copy}\").count()\n","            self.assertEqual(record_count,3)\n","            sql(f\"DROP TABLE IF EXISTS {table_name_copy}\")\n","        except Exception as ex:\n","            msg={'command':'INSERT INTO TABLE','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","        finally:\n","            sql(f\"DROP TABLE IF EXISTS {table_name_copy}\")\n","\n","    def test_dml_insert_004_using_from(self):\n","        \"\"\"insert using from\"\"\"\n","        table_name_cp_from=f\"{PREFIX}_student_insert_table_cp_from_{SUFFIX}\"\n","        try:\n","            table_name_copy_sql = f\"CREATE TABLE {table_name_cp_from} (id INT, name STRING) \\\n","                        PARTITIONED BY (age INT)\"\n","            sql(table_name_copy_sql)\n","            sql(f\"INSERT INTO {table_name_cp_from} FROM {self.table_name} SELECT id, name,age WHERE id=1\")\n","            record_count=sql(f\"SELECT * FROM {table_name_cp_from}\").count()\n","            self.assertEqual(record_count,1)\n","        except Exception as ex:\n","            msg={'command':'INSERT INTO FROM SELECT','status':'fail'}\n","            self.fail(f\"{msg}\")\n","        \n","        finally:\n","            sql(f\"DROP TABLE IF EXISTS {table_name_cp_from}\")\n","    \n","    def test_dml_insert_005_with_column_list_with_part_spec(self):\n","        \"\"\"insert with both a partition spec and a column list\"\"\"\n","        table_name_col_part=f\"{PREFIX}_student_insert_table_col_list_{SUFFIX}\"\n","        try:\n","            table_name_col_part_sql = f\"CREATE TABLE {table_name_col_part} (id INT, name STRING) \\\n","                        PARTITIONED BY (age INT)\"\n","            sql(table_name_col_part_sql)\n","            sql(f\"INSERT INTO {table_name_col_part} PARTITION (age=20) (id,name) VALUES (1,'a'),(2,'b')\")\n","            record_count=sql(f\"SELECT * FROM {table_name_col_part}\").count()\n","            self.assertEqual(record_count,2)\n","        except Exception as ex:\n","            msg={'command':'INSERT With Column List and Partition Spec','status':'fail'}\n","            self.fail(f\"{msg}\")\n","        \n","        finally:\n","            sql(f\"DROP TABLE IF EXISTS {table_name_col_part}\")\n","        \n","    \n","    def test_dml_insert_006_with_typed_date_part_col(self):\n","        \"\"\"insert Using a Typed Date Literal for a Partition Column Value\"\"\"\n","        table_name_typed_date_part=f\"{PREFIX}_student_insert_table_date_part_{SUFFIX}\"\n","        try:\n","            table_name_copy_sql = f\"CREATE TABLE {table_name_typed_date_part} (id INT, name STRING) \\\n","                        PARTITIONED BY (birthday DATE)\"\n","            sql(table_name_copy_sql)\n","            sql(f\"INSERT INTO {table_name_typed_date_part} PARTITION (birthday = date'2019-01-02') (id,name) VALUES (1,'a'),(2,'b')\")\n","            record_count=sql(f\"SELECT * FROM {table_name_typed_date_part}\").count()\n","            self.assertEqual(record_count,2)\n","        except Exception as ex:\n","            msg={'command':'INSERT Using a Typed Date Literal for a Partition Column Value','status':'fail'}\n","            self.fail(f\"{msg}\")\n","        \n","        finally:\n","            sql(f\"DROP TABLE IF EXISTS {table_name_typed_date_part}\")\n","    \n","    def test_dml_insert_007_overwrite_spark_format(self):\n","        \"\"\"insert overwrite for Spark format\"\"\"\n","        output_path = f\"{DIRECTORY_PATH}/{TEST_RUN_ID}/spark\"\n","        try:\n","            df = sql(f\"SELECT * FROM {self.table_name}\")\n","            df.write.parquet(f'{output_path}')\n","            sql_cmd = f\"INSERT OVERWRITE DIRECTORY \\\n","                    USING parquet \\\n","                    OPTIONS ('path' '{output_path}') \\\n","                  SELECT * FROM {self.table_name};\"\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'INSERT OVERWRITE DIRECTORY USING parquet','status':'fail'}\n","            self.fail(f\"{msg}\")\n","        \n","    def test_dml_insert_008_overwrite_hive_format(self):\n","        \"\"\"insert overwrite for hive format\"\"\"\n","        output_path = f\"{DIRECTORY_PATH}/{TEST_RUN_ID}/hive\"\n","        try:\n","            df = sql(f\"SELECT * FROM {self.table_name}\")\n","            df.write.parquet(f'{output_path}')\n","            sql_cmd = f\"INSERT OVERWRITE LOCAL DIRECTORY '{output_path}'\\\n","                    STORED AS orc \\\n","                  SELECT * FROM {self.table_name};\"\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'INSERT OVERWRITE LOCAL DIRECTORY STORED AS orc','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_dml_insert_009_load_into_table(self):\n","        \"\"\"insert LOAD INTO TABLE\"\"\"\n","        input_path = f\"{DIRECTORY_PATH}/{TEST_RUN_ID}/hive\"\n","        table_name_load=f\"{PREFIX}_student_insert_table_load_{SUFFIX}\"\n","        table_sql = f\"CREATE TABLE {table_name_load} (id INT, name STRING, age INT) USING Hive\"\n","        try:\n","            sql(table_sql)\n","            sql_cmd = f\"LOAD DATA LOCAL INPATH '{input_path}' OVERWRITE INTO TABLE {table_name_load}\"\n","            sql(sql_cmd)\n","            df = sql(f\"SELECT * FROM {table_name_load}\").count()\n","            self.assertEqual(record_count,2)\n","        except Exception as ex:\n","            msg={'command':'LOAD DATA LOCAL INTO TABLE','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","        finally:\n","            sql(f\"DROP TABLE IF EXISTS {table_name_load}\")\n","\n","    @classmethod\n","    def tearDownClass(cls):\n","        \"\"\"tear down\"\"\"\n","        sql(f\"DROP TABLE IF EXISTS {cls.table_name}\")\n","\n","\n","# TODO: Add Hive complex datatype, avro and "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"31f8bd52-55be-4e19-907f-c8bc80b982af"},{"cell_type":"markdown","source":["### Execute Test Case"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fa8e605d-7c55-47aa-802a-b8d2f60b500f"},{"cell_type":"code","source":["import io\n","import xmlrunner\n","loader = unittest.TestLoader()\n","suite  = unittest.TestSuite()\n","\n","# add tests to the test suite\n","suite.addTests(loader.loadTestsFromTestCase(DMLInsertTableTest))\n","\n","\n","# initialize a runner, pass it your suite and run it\n","out = io.BytesIO()\n","runner = xmlrunner.XMLTestRunner(output=out)\n","result = runner.run(suite)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"92d0b3bf-2cc8-4f59-b37d-3aab8b1effdc"},{"cell_type":"markdown","source":["## Report for Test"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3935d6b2-8879-4bcb-b25a-dfc100be48c1"},{"cell_type":"code","source":["from pyspark.sql.functions import col, explode,isnull,from_json, expr, to_json, coalesce, lit\n","from pyspark.sql.types import StructType,StructField,StringType\n","import json\n","import xmltodict\n","\n","dict_result=xmltodict.parse(out.getvalue())\n","json_result = json.loads(json.dumps(dict_result,indent=4).replace('@',''))\n","test_suites=json_result['testsuites']['testsuite']\n","\n","df = spark.read.json(sc.parallelize([test_suites]))\n","fail_schema = StructType([\n","  StructField(\"command\", StringType(), True),\n","  StructField(\"status\", StringType(),  True)\n","])\n","\n","test_cases_df= df.withColumn('ts',explode('testcase')).drop(col('testcase'))\n","schema_string = test_cases_df.schema.simpleString()\n","if \"failure:\" in test_cases_df.schema.simpleString():\n"," explode_df= test_cases_df.withColumn('fail',from_json(col('ts.failure.message'),fail_schema)).drop(col('ts.failure'))\n","else:\n"," explode_df= test_cases_df.withColumn(\"fail\",from_json(expr(\"to_json(named_struct('command', '', 'status', 'pass'))\"),fail_schema))\n"," \n","df_test_result=explode_df.select(col(\"errors\").alias(\"errorInSuite\"),col(\"failures\").alias(\"failedInSuite\"),col(\"name\").alias(\"suitename\"),\\\n","      \"skipped\",col(\"tests\").alias(\"totalTest\"), col(\"timestamp\").alias(\"executionTime\"),col(\"ts.name\").alias(\"testCaseName\"), \\\n","       col(\"ts.time\").alias(\"testCaseTime\"),coalesce(col(\"fail.command\"), lit(\"\")).alias(\"failcommand\"),coalesce(col(\"fail.status\"), lit(\"pass\")).alias(\"status\"))\n","\n","if (len(storage_account)>0 and len(result_container)>0):\n","    # save result to storage\n","    storage_path = f\"abfs://{result_container}@{storage_account}.dfs.core.windows.net/{TEST_RUN_ID}/{PLATFORM}/{TEST_SUITE}\"\n","    # write raw results\n","    df.write.parquet(f\"{storage_path}/{RAW_RESULT_FILE_NAME}\")\n","    # write transformed results\n","    df_test_result.write.parquet(f\"{storage_path}/{RESULT_FILE_NAME}\") \n","else:\n","    print(\"configure storage path to store results\")\n","    df_test_result.show(200,False)    "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0d350743-2897-493f-92f1-5bb0c714972c"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"host":{"synapse_widget":{"token":"d82f58d3-0680-4b63-98be-da4a7c35824d","state":{}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"cbf623b8-20a2-4a96-92cb-6f6b7d7f1e3d"}],"default_lakehouse":"cbf623b8-20a2-4a96-92cb-6f6b7d7f1e3d","default_lakehouse_name":"testsparksql_statements","default_lakehouse_workspace_id":"4f212d69-90d6-4648-b0c3-70dbd6d9a307"}}},"nbformat":4,"nbformat_minor":5}