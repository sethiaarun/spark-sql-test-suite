{"cells":[{"cell_type":"markdown","id":"31b4a0ed-dc8f-4e28-a906-448936371cc8","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Test Cases for DDL used by Apache Spark\n","**Spark SQL 3.3 DDL and DML Reference** \n","https://spark.apache.org/docs/3.3.0/sql-ref-syntax.html#ddl-statements\n","\n","**<mark>Excludes</mark>:**\n","1. Use Database\n","2. Functions (Create, Drop Function)\n","\n","To store these results configure data <mark>**storage account and container**</mark>."]},{"cell_type":"code","execution_count":null,"id":"84095056-4467-44ac-a636-6030ca9f3e57","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["!pip install unittest-xml-reporting xmltodict"]},{"cell_type":"markdown","id":"dac28517-24d2-4bc5-a073-3a27a1707bb1","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Configure Result Storage Location"]},{"cell_type":"code","execution_count":null,"id":"16c2aede-6051-4bed-bbb7-054e894ac2de","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["storage_account=\"\"\n","result_container=\"\""]},{"cell_type":"markdown","id":"1f170c66-dbe2-4738-b396-ce2c7f470bea","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Initialize Common Variables for the test run"]},{"cell_type":"code","execution_count":null,"id":"9253f197-5e4d-41bb-b38d-ce18739ecf0e","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import time\n","\n","# Don't change these variables\n","TEST_SUITE= \"SPARK_SQL_DDL\"\n","RESULT_FILE_NAME=\"ddl_test_result.parquet\"\n","RAW_RESULT_FILE_NAME=\"raw_ddl_test_result.parquet\"\n","# Test Run ID\n","TEST_RUN_ID= round(time.time()*1000)\n","# Test platform\n","PLATFORM = \"nameoftheplatform\"\n","# Prefix for all tables\n","PREFIX = PLATFORM\n","SUFFIX = TEST_RUN_ID\n","# Spark SQL function\n","sql=spark.sql"]},{"cell_type":"markdown","id":"51c090ab-3455-41bb-b267-688b01511007","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Set Common Spark Configurations"]},{"cell_type":"code","execution_count":null,"id":"5f8f0f5b-6d75-42df-be1f-f38f37bcbf4a","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")"]},{"cell_type":"markdown","id":"e5f5253e-6dae-4e06-8318-bbaeef2f7f80","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### DDL - Database\n","1. Create database\n","2. Alter database - DB Properties\n","3. Describe database\n","4. Alter database location"]},{"cell_type":"code","execution_count":null,"id":"83961d49-2be4-4922-8a56-99db5cea7e95","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import unittest\n","\n","TEST_DATABASE_NAME = f\"sparksql_spec_db_{SUFFIX}\"\n","DB_LOCATION = f\"Files/sparksql_spec_ref_synapse/{SUFFIX}\"\n","\n","class DDLDatabaseTest(unittest.TestCase):\n","    \"\"\"Database related test cases\"\"\"\n","    def test_ddl_database_00_create_database(self):\n","        \"\"\"create database at location\"\"\"\n","        sql_cmd = f\"CREATE DATABASE IF NOT EXISTS {PREFIX}_{TEST_DATABASE_NAME}_{SUFFIX} COMMENT 'This is spark sql specification database' LOCATION '{DB_LOCATION}' \\\n","                        WITH DBPROPERTIES (ID=001, Name='User')\"\n","        try:\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'CREATE DATABASE','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_database_01_alter_database_loction(self):\n","        \"\"\"alter database at location\"\"\"\n","        sql_cmd = f\"ALTER DATABASE {PREFIX}_{TEST_DATABASE_NAME}_{SUFFIX} SET LOCATION 'sparksql_spec_ref_synapse_newlocation'\"\n","        try:\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'ALTER DATABASE LOCATION','status':'fail'}\n","            self.fail(f\"{msg}\")\n","        \n","    def test_ddl_database_02_alter_database_prop(self):\n","        \"\"\"alter database properties\"\"\"\n","        sql_cmd = f\"ALTER DATABASE {PREFIX}_{TEST_DATABASE_NAME}_{SUFFIX} SET DBPROPERTIES ('Edited-by' = 'NewUser', 'Edit-date' = '10/10/2023')\"\n","        try:\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'ALTER DATABASE PROP','status':'fail'}\n","            self.fail(f\"{msg}\")"]},{"cell_type":"markdown","id":"06a69bcf-b013-4dc7-bb2c-be4420ed1a98","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## DDL - Create Table \n","1. Using data source\n","2. Using Hive Format\n","3. Using Like"]},{"cell_type":"code","execution_count":null,"id":"31f8bd52-55be-4e19-907f-c8bc80b982af","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import unittest\n","\n","class DDLCreateTableTest(unittest.TestCase):\n","\n","    list_of_tables_to_remove=[]\n","    parquet_table_name=f\"{PREFIX}_student_parquet_{SUFFIX}\"\n","    csv_table_name= f\"{PREFIX}_student_csv_{SUFFIX}\"\n","    orc_table_name = f\"{PREFIX}_Student_hive_orc_{SUFFIX}\"\n","\n","    \"\"\"Create table test cases\"\"\"\n","    def test_ddl_create_table_001_create_table_datasource(self):\n","        table_name = f\"{PREFIX}_student_delta_table_{SUFFIX}\"\n","        \"\"\"CREATE DATASOURCE DELTA TABLE\"\"\"\n","        sql_cmd = f\"CREATE TABLE {table_name} (id INT, name STRING, age INT) \\\n","                     USING DELTA PARTITIONED BY (age)\"\n","        try:\n","            self.list_of_tables_to_remove.append(table_name)\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'CREATE DATASOURCE DELTA TABLE','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_create_table_002_create_table_partition_bucketed_parquet(self):\n","        \"\"\"Create partitioned and bucketed table parquet\"\"\"\n","        sql_cmd = f\"CREATE TABLE {self.parquet_table_name} (id INT, name STRING, age INT) \\\n","                      USING parquet PARTITIONED BY (age) CLUSTERED BY (Id) INTO 4 buckets\"\n","        try:\n","            self.list_of_tables_to_remove.append(self.parquet_table_name)\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'CREATE TABLE USING parquet PARTITIONED BY CLUSTERED BY buckets','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_create_table_003_create_table_partition_bucketed_csv(self):\n","        \"\"\"Create partitioned and bucketed table csv\"\"\"\n","        sql_cmd = f\"CREATE TABLE {self.csv_table_name} (id INT, name STRING, age INT) \\\n","                   USING csv PARTITIONED BY (age) CLUSTERED BY (Id) INTO 4 buckets\"\n","        try:\n","            self.list_of_tables_to_remove.append(self.csv_table_name)\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'CREATE TABLE USING csv PARTITIONED BY CLUSTERED BY buckets','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_create_table_004_create_table_ctas_cte(self):\n","        \"\"\"Create bucketed table through CTAS and CTE using parquet table\"\"\"\n","        table_name=f\"{PREFIX}_student_bucket_ctas_cte_{SUFFIX}\"\n","        sql_cmd = f\"CREATE TABLE {table_name} USING parquet CLUSTERED BY (id) INTO 4 buckets ( \\\n","                WITH {PREFIX}_tmpTable_{SUFFIX} AS ( \\\n","                    SELECT * FROM {self.parquet_table_name} WHERE id > 100 \\\n","                ) \\\n","                SELECT * FROM {PREFIX}_tmpTable_{SUFFIX} \\\n","              );\"\n","        try:\n","            self.list_of_tables_to_remove.append(table_name)\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'CREATE TABLE USING parquet CLUSTERED BY through CTAS and CTE','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_create_table_005_create_table_like_from_existing_table(self):\n","        \"\"\"Create table like using existing table\"\"\"\n","        table_name= f\"{PREFIX}_Student_Dupli_{SUFFIX}\"\n","        sql_cmd = f\"CREATE TABLE {table_name} like {self.parquet_table_name};\"\n","        try:\n","            self.list_of_tables_to_remove.append(table_name)\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'CREATE TABLE like using existing table','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_create_table_006_create_table_like_from_existing_table(self):\n","        \"\"\"Create table like using existing table with datasource\"\"\"\n","        table_name=f\"{PREFIX}_Student_Dupli_datasource_{SUFFIX}\"\n","        sql_cmd = f\"CREATE TABLE {table_name} like {self.csv_table_name} using CSV;\"\n","        try:\n","            self.list_of_tables_to_remove.append(table_name)\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'CREATE TABLE like using existing table using datasource','status':'fail'}\n","            self.fail(f\"{msg}\")\n","    \n","    def test_ddl_create_table_007_create_table_like_row_format(self):\n","        \"\"\"Create table like using existing table with rowformat\"\"\"\n","        table_name=f\"{PREFIX}_student_Dupli_like_rowformat_{SUFFIX}\"\n","        sql_cmd = f\"CREATE TABLE {table_name} like {self.csv_table_name} \\\n","                  ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \\\n","                     STORED AS TEXTFILE TBLPROPERTIES ('createdby'='Test');\"\n","        try:\n","            self.list_of_tables_to_remove.append(table_name)\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'CREATE TABLE like using existing table with row format','status':'fail'}\n","            self.fail(f\"{msg}\")\n","    \n","    def test_ddl_create_table_008_create_table_hive_format_orc(self):\n","        \"\"\"Create table Hive format ORC\"\"\"\n","        sql_cmd = f\"CREATE TABLE {self.orc_table_name}(id INT, name STRING, age INT) \\\n","                    COMMENT 'this is a comment' \\\n","                    TBLPROPERTIES ('user'='testcases') \\\n","                    STORED AS ORC;\"\n","        try:\n","            self.list_of_tables_to_remove.append(self.orc_table_name)\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'CREATE TABLE Format ORC','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_create_table_009_create_table_hive_format_orc_from_another_table(self):\n","        \"\"\"Create table Hive format ORC use data from another table\"\"\"\n","        table_name =f\"{PREFIX}_Student_hive_orc_copy_{SUFFIX}\"\n","        sql_cmd = f\"CREATE TABLE {table_name} STORED AS ORC \\\n","                    AS SELECT * FROM {self.orc_table_name};\"\n","        try:\n","            self.list_of_tables_to_remove.append(table_name)\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'CREATE TABLE ORC from another table','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_create_table_010_create_table_hive_format_orc_partitioned_table(self):\n","        \"\"\"Create table Hive format ORC partitioned table\"\"\"\n","        table_name = f\"{PREFIX}_Student_hive_orc_partition_{SUFFIX}\"\n","        sql_cmd = f\"CREATE TABLE {table_name} (id INT, name STRING) \\\n","                    STORED AS ORC \\\n","                    PARTITIONED BY (age INT);\"\n","        try:\n","            self.list_of_tables_to_remove.append(table_name)\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'Hive:CREATE TABLE ORC with partition','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_create_table_011_create_table_hive_format_text(self):\n","        \"\"\"Create table Hive Row Format and file format\"\"\"\n","        table_name = f\"{PREFIX}_Student_hive_text_{SUFFIX}\"\n","        sql_cmd = f\"CREATE TABLE {table_name} (id INT, name STRING, age INT) \\\n","                    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\\\n","                     STORED AS TEXTFILE;\"\n","        try:\n","            self.list_of_tables_to_remove.append(table_name)\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'CREATE TABLE Row Format and file format','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_create_table_012_create_table_hive_format_cluster_by(self):\n","        \"\"\"Create table Hive clustered_by bucket table without SORTED BY, it is working with Apache Spark\"\"\"\n","        # for example CREATE TABLE test (id INT, name STRING, age INT) CLUSTERED BY (ID) INTO 4 BUCKETS STORED AS ORC\"\n","        table_name = f\"{PREFIX}_student_clustered_by_{SUFFIX}\"\n","        sql_cmd = f\"CREATE TABLE {table_name} (id INT, name STRING, age INT) \\\n","                    CLUSTERED BY (ID) \\\n","                    INTO 4 BUCKETS \\\n","                    STORED AS ORC;\"\n","        try:\n","            self.list_of_tables_to_remove.append(table_name)\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'Create table clustered_by bucket table without SORTED BY','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_create_table_013_create_table_hive_format_cluster_by_sorted_by(self):\n","        \"\"\"Create table Hive clustered_by bucket table with SORTED BY\"\"\"\n","        table_name=f\"{PREFIX}_student_clustered_by_sorted_by{SUFFIX}\"\n","        sql_cmd = f\"CREATE TABLE {table_name} (id INT, name STRING) \\\n","                    PARTITIONED BY (YEAR STRING) \\\n","                    CLUSTERED BY (ID, NAME) \\\n","                    SORTED BY (ID ASC) \\\n","                    INTO 3 BUCKETS \\\n","                    STORED AS ORC;\"\n","        try:\n","            self.list_of_tables_to_remove.append(table_name)\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'Create table clustered_by bucket table with SORTED BY','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    @classmethod\n","    def tearDownClass(cls):\n","        \"\"\"tear down\"\"\"\n","        for table in cls.list_of_tables_to_remove:\n","            try:\n","                sql(f\"DROP TABLE IF EXISTS {table}\")\n","            except Exception as e:\n","                print(f\"table drop failed: {table}\")"]},{"cell_type":"markdown","id":"10613d2f-8c8a-4519-a608-d85e2b65fec4","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### DDL - Alter Table"]},{"cell_type":"code","execution_count":null,"id":"e54f3672-4166-4fb3-b824-18a5d3b5d69e","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import unittest\n","from pyspark.sql.utils import AnalysisException\n","\n","class DDLAlterTableTest(unittest.TestCase):\n","    \"\"\"Alter table test cases\"\"\"\n","    \n","    table_name = f\"{PREFIX}_student_alter_table_{SUFFIX}\"\n","    rename_table = f\"{PREFIX}_student_alter_table_{SUFFIX}_copy\"\n","    list_of_tables_to_remove=[]\n","\n","    @classmethod\n","    def setUpClass(cls):\n","        sql_cmd = f\"CREATE TABLE {cls.table_name} (id INT, name STRING) \\\n","                        PARTITIONED BY (age INT)\"\n","        try:\n","            sql(sql_cmd)\n","            sql(f\"INSERT INTO {cls.table_name} VALUES \\\n","                 (1,'a',10),(2,'b',20),(3,'c',30);\")\n","        except Exception as ex:\n","            msg={'command':'AlterTable:Setup failed','status':'fail'}\n","            cls.fail(f\"{msg}\")\n","\n","    \n","    def test_ddl_altertable_001_rename_table_name(self):\n","        \"\"\"Rename Table\"\"\"\n","        sql_cmd = f\"ALTER TABLE {self.table_name} RENAME TO {self.rename_table}\"\n","        try:\n","            sql(sql_cmd)\n","            # this table to be removed during teardown\n","            self.list_of_tables_to_remove.append(self.rename_table)\n","            record_count=sql(f\"SELECT * FROM {self.rename_table}\").count()\n","            self.assertEqual(record_count,3)\n","            num_of_partition=sql(f\"SHOW PARTITIONS {self.rename_table}\").where(\"partition='age=10'\").count()\n","            self.assertEqual(num_of_partition,1)\n","        except Exception as ex:\n","            msg={'command':'ALTER TABLE RENAME TO','status':'fail'}\n","            self.list_of_tables_to_remove.append(self.table_name)\n","            self.fail(f\"{msg}\")\n","    \n","    def test_ddl_altertable_002_rename_partition(self):\n","        \"\"\"Rename Table Partition after renaming Table Name, this scenario is working with open source version\"\"\"\n","        sql_cmd = f\"ALTER TABLE {self.rename_table} PARTITION (age='10') RENAME TO PARTITION (age='15')\"\n","        try:\n","            sql(sql_cmd)\n","            num_of_partition=sql(f\"SHOW PARTITIONS {self.rename_table}\").where(\"partition='age=10'\").count()\n","            self.assertEqual(num_of_partition,0)\n","        except Exception as ex:\n","            msg={'command':'ALTER TABLE PARTITION RENAME TO PARTITION','status':'fail'}\n","            self.list_of_tables_to_remove.append(self.table_name)\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_altertable_003_add_new_partitions(self):\n","        \"\"\"Add multiple new partition\"\"\"\n","        sql_cmd = f\"ALTER TABLE {self.rename_table} ADD IF NOT EXISTS PARTITION (age=18)\"\n","        try:\n","            sql(sql_cmd)\n","            num_of_partition=sql(f\"SHOW PARTITIONS {self.rename_table}\").where(\"partition='age=18'\").count()\n","            self.assertEqual(num_of_partition,2)\n","        except Exception as ex:\n","            msg={'command':'ALTER TABLE ADD PARTITION','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_altertable_004_drop_new_partition(self):\n","        \"\"\"Remove a partition\"\"\"\n","        sql_cmd = f\"ALTER TABLE {self.rename_table} DROP IF EXISTS PARTITION (age=18)\"\n","        try:\n","            sql(sql_cmd)\n","            num_of_partition=sql(f\"SHOW PARTITIONS {self.rename_table}\").where(\"partition='age=18'\").count()\n","            self.assertEqual(num_of_partition,0)\n","        except Exception as ex:\n","            msg={'command':'ALTER TABLE DROP PARTITION','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_altertable_005_add_column(self):\n","        \"\"\"Add column \"\"\"\n","        sql_cmd = f\"ALTER TABLE {self.rename_table} ADD COLUMNS (LastName string, country string)\"\n","        try:\n","            sql(sql_cmd)\n","            col_exist=sql(f\"DESC {self.rename_table}\").where(\"col_name='LastName'\").count()\n","            self.assertEqual(col_exist,1)\n","        except Exception as ex:\n","            msg={'command':'ALTER TABLE ADD COLUMNS','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_altertable_006_alter_column_comments(self):\n","        \"\"\"Rename column comments\"\"\"\n","        sql_cmd = f\"ALTER TABLE {self.rename_table} ALTER COLUMN LastName COMMENT 'new comment'\"\n","        try:\n","            sql(sql_cmd)\n","            col_exist=sql(f\"DESC {self.rename_table}\").where(\"comment='new comment' AND col_name='LastName'\").count()\n","            self.assertEqual(col_exist,1)\n","        except Exception as ex:\n","            msg={'command':'ALTER TABLE ALTER COLUMN COMMENT ','status':'fail'}\n","            self.fail(f\"{msg}\")\n","    \n","    def test_ddl_altertable_007_drop_table(self):\n","        \"\"\"drop table\"\"\"\n","        table_name = f\"{PREFIX}_drop_table_test_{SUFFIX}\"\n","        sql_cmd = f\"CREATE TABLE {table_name} (a STRING);\"\n","        try:\n","            sql(sql_cmd)\n","            table_exist=sql(f\"SHOW TABLES\").where(f\"tableName='{table_name}'\").count()\n","            self.assertEqual(table_exist,1)\n","            sql(f\"DROP TABLE {table_name}\")\n","            table_exist=sql(f\"SHOW TABLES\").where(f\"tableName='{table_name}'\").count()\n","            self.assertEqual(table_exist,0)\n","        except Exception as ex:\n","            msg={'command':'DROP TABLE','status':'fail'}\n","            self.list_of_tables_to_remove.append(table_name)\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_altertable_008_remove_column(self):\n","        \"\"\"Remove column supported with v2 tables.\"\"\"\n","        sql_cmd = f\"ALTER TABLE {self.rename_table} DROP COLUMN (LastName)\"\n","        with self.assertRaises(AnalysisException,msg=\"ALTER TABLE DROP COLUMN - not supported with V1 table\"):\n","            sql(sql_cmd)\n","       \n","    def test_ddl_altertable_009_rename_column(self):\n","        \"\"\"Rename column - Supported only for V2 tables\"\"\"\n","        sql_cmd = f\"ALTER TABLE {self.rename_table} RENAME COLUMN LastName To LName\"\n","        with self.assertRaises(AnalysisException,msg=\"ALTER TABLE RENAME COLUMN - not supported with V1 table\"):\n","            sql(sql_cmd)\n","\n","   \n","    def test_ddl_altertable_010_replace_column(self):\n","        \"\"\"Replace columns supported with v2 tables. \"\"\"\n","        sql_cmd = f\"ALTER TABLE {self.rename_table} REPLACE COLUMNS (name string, ID int COMMENT 'new comment')\"\n","        with self.assertRaises(AnalysisException,msg=\"ALTER TABLE REPLACE COLUMNS\"):\n","            sql(sql_cmd)\n","\n","    @classmethod\n","    def tearDownClass(cls):\n","        \"\"\"tear down\"\"\"\n","        for table in cls.list_of_tables_to_remove:\n","            try:\n","                sql(f\"DROP TABLE IF EXISTS {table}\")\n","            except Exception as e:\n","                print(f\"table drop failed: {table}\")\n","\n"]},{"cell_type":"markdown","id":"abfdbb78-fd70-478d-823a-259c4290ebc3","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### DDL - Views"]},{"cell_type":"code","execution_count":null,"id":"ce097b13-c34d-4eba-8e25-e6dd3da42ac4","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import unittest\n","from pyspark.sql.utils import AnalysisException\n","\n","class DDLViewTest(unittest.TestCase):\n","    \"\"\"View test cases\"\"\"\n","    \n","    table_name = f\"{PREFIX}_student_table_for_view_{SUFFIX}\"\n","    view_name = f\"{PREFIX}_student_table_view_{SUFFIX}\"\n","   \n","    @classmethod\n","    def setUpClass(cls):\n","        sql_cmd = f\"CREATE TABLE {cls.table_name} (id INT, fname STRING, lname STRING) \\\n","                        PARTITIONED BY (age INT)\"\n","        try:\n","            sql(sql_cmd)\n","            sql(f\"INSERT INTO {cls.table_name} VALUES \\\n","                 (1,'fa','la',10),(2,'fb','lb',20),(3,'fc','lc',30);\")\n","        except Exception as ex:\n","            msg={'command':'View:Setup failed','status':'fail'}\n","            cls.fail(f\"{msg}\")\n","    \n","    def test_ddl_view_001_create_view(self):\n","        \"\"\"create view\"\"\"\n","        sql_cmd = f\"CREATE OR REPLACE VIEW  {self.view_name} (fname COMMENT 'First Name', lName COMMENT 'Last Name') \\\n","                    COMMENT 'View for student table' \\\n","                    AS SELECT fname, lname FROM {self.table_name};\"\n","        try:\n","            sql(sql_cmd)\n","        except Exception as ex:\n","            msg={'command':'CREATE OR REPLACE VIEW','status':'fail'}\n","            self.fail(f\"{msg}\")\n","    \n","    def test_ddl_view_002_show_view(self):\n","        \"\"\"show view\"\"\"\n","        try:\n","            view_exist=sql(f\"SHOW VIEWS\").filter(f\"viewName='{self.view_name}'\").count()\n","            self.assertEqual(view_exist,1)\n","        except Exception as ex:\n","            msg={'command':'SHOW VIEWS','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_view_003_show_view_like(self):\n","        \"\"\"show view\"\"\"\n","        try:\n","            view_exist=sql(f\"SHOW VIEWS LIKE '{self.view_name}*'\").count()\n","            self.assertEqual(view_exist,1)\n","        except Exception as ex:\n","            msg={'command':'SHOW VIEWS LIKE','status':'fail'}\n","            self.fail(f\"{msg}\")\n","\n","    def test_ddl_view_004_drop_view(self):\n","        \"\"\"drop view\"\"\"\n","        try:\n","            sql(f\"DROP VIEW {self.view_name}\")\n","            view_exist=sql(f\"SHOW VIEWS\").filter(f\"viewName='{self.view_name}'\").count()\n","            self.assertEqual(view_exist,0)\n","        except Exception as ex:\n","            msg={'command':'DROP VIEW','status':'fail'}\n","            self.fail(f\"{msg}\")    \n","    \n","    \n","    @classmethod\n","    def tearDownClass(cls):\n","        \"\"\"tear down\"\"\"\n","        try:\n","            sql(f\"DROP TABLE IF EXISTS {cls.table_name}\")\n","        except Exception as e:\n","            print(f\"table drop failed: {cls.table_name}\")\n"]},{"cell_type":"markdown","id":"f4f3aa99-e110-4309-8edc-0642a5fb3ccc","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### DDL - Misc Table Command \n","1. Truncate Table\n","2. Repair Table"]},{"cell_type":"code","execution_count":null,"id":"64ee20ee-ee2b-428b-8064-a532274ff70a","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import unittest\n","from pyspark.sql.utils import AnalysisException\n","\n","class DDLMiscTableOperations(unittest.TestCase):\n","    \"\"\"miscellaneous table operations\"\"\"\n","    \n","    table_name = f\"{PREFIX}_student_table_for_misc_operations_{SUFFIX}\"\n","   \n","    @classmethod\n","    def setUpClass(cls):\n","        sql_cmd = f\"CREATE TABLE {cls.table_name} (id INT, fname STRING, lname STRING) \\\n","                        PARTITIONED BY (age INT)\"\n","        try:\n","            sql(sql_cmd)\n","            sql(f\"INSERT INTO {cls.table_name} VALUES \\\n","                 (1,'fa','la',10),(11,'f1a','l1a',10),(2,'fb','lb',20),(3,'fc','lc',30);\")\n","        except Exception as ex:\n","            msg={'command':'MiscTableOps:Setup failed','status':'fail'}\n","            cls.fail(f\"{msg}\")\n","    \n","    def test_ddl_misc_table_001_truncate_table_partition(self):\n","        \"\"\"truncate table\"\"\"\n","        sql_cmd = f\"TRUNCATE TABLE {self.table_name} PARTITION (age=20);\"\n","        try:\n","            sql(sql_cmd)\n","            num_of_rows=sql(f\"SELECT * FROM {self.table_name}\").where(\"age=20\").count()\n","            self.assertEqual(num_of_rows,0)\n","        except Exception as ex:\n","            msg={'command':'TRUNCATE TABLE PARTITION','status':'fail'}\n","            self.fail(f\"{msg}\")\n","    \n","    def test_ddl_misc_table_002_repair_table(self):\n","        \"\"\"repair table recovers all the partitions in the directory of a table\"\"\"\n","        try:\n","            sql(f\"MSCK REPAIR TABLE {self.table_name} DROP PARTITIONS\")\n","        except Exception as ex:\n","            msg={'command':'MSCK REPAIR TABLE DROP PARTITIONS','status':'fail'}\n","            self.fail(f\"{msg}\")\n","    \n","    \n","    @classmethod\n","    def tearDownClass(cls):\n","        \"\"\"tear down\"\"\"\n","        try:\n","            sql(f\"DROP TABLE IF EXISTS {cls.table_name}\")\n","        except Exception as e:\n","            print(f\"table drop failed: {cls.table_name}\")"]},{"cell_type":"markdown","id":"fa8e605d-7c55-47aa-802a-b8d2f60b500f","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Execute Test Case"]},{"cell_type":"code","execution_count":null,"id":"92d0b3bf-2cc8-4f59-b37d-3aab8b1effdc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import io\n","import xmlrunner\n","loader = unittest.TestLoader()\n","suite  = unittest.TestSuite()\n","\n","# add tests to the test suite\n","suite.addTests(loader.loadTestsFromTestCase(DDLDatabaseTest))\n","suite.addTests(loader.loadTestsFromTestCase(DDLCreateTableTest))\n","suite.addTests(loader.loadTestsFromTestCase(DDLViewTest))\n","suite.addTests(loader.loadTestsFromTestCase(DDLAlterTableTest))\n","suite.addTest(loader.loadTestsFromTestCase(DDLMiscTableOperations))\n","\n","\n","# initialize a runner, pass it your suite and run it\n","out = io.BytesIO()\n","runner = xmlrunner.XMLTestRunner(output=out)\n","result = runner.run(suite)"]},{"cell_type":"markdown","id":"3935d6b2-8879-4bcb-b25a-dfc100be48c1","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Report for Test"]},{"cell_type":"code","execution_count":162,"id":"0d350743-2897-493f-92f1-5bb0c714972c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql.functions import col, explode,isnull,from_json, expr, to_json, coalesce, lit\n","from pyspark.sql.types import StructType,StructField,StringType\n","import json\n","import xmltodict\n","\n","dict_result=xmltodict.parse(out.getvalue())\n","json_result = json.loads(json.dumps(dict_result,indent=4).replace('@',''))\n","test_suites=json_result['testsuites']['testsuite']\n","\n","df = spark.read.json(sc.parallelize([test_suites]))\n","fail_schema = StructType([\n","  StructField(\"command\", StringType(), True),\n","  StructField(\"status\", StringType(),  True)\n","])\n","\n","test_cases_df= df.withColumn('ts',explode('testcase')).drop(col('testcase'))\n","if \"failure:\" in test_cases_df.schema.simpleString():\n"," explode_df= test_cases_df.withColumn('fail',from_json(col('ts.failure.message'),fail_schema)).drop(col('ts.failure'))\n","else:\n"," explode_df= test_cases_df.withColumn(\"fail\",from_json(expr(\"to_json(named_struct('command', '', 'status', 'pass'))\"),fail_schema))\n"," \n","df_test_result=explode_df.select(col(\"errors\").alias(\"errorInSuite\"),col(\"failures\").alias(\"failedInSuite\"),col(\"name\").alias(\"suitename\"),\\\n","      \"skipped\",col(\"tests\").alias(\"totalTest\"), col(\"timestamp\").alias(\"executionTime\"),col(\"ts.name\").alias(\"testCaseName\"), \\\n","       col(\"ts.time\").alias(\"testCaseTime\"),coalesce(col(\"fail.command\"), lit(\"\")).alias(\"failcommand\"),coalesce(col(\"fail.status\"), lit(\"pass\")).alias(\"status\"))\n","\n","if (len(storage_account)>0 and len(result_container)>0):\n","    # save result to storage\n","    storage_path = f\"abfs://{result_container}@{storage_account}.dfs.core.windows.net/{TEST_RUN_ID}/{PLATFORM}/{TEST_SUITE}\"\n","    # write raw results\n","    df.write.parquet(f\"{storage_path}/{RAW_RESULT_FILE_NAME}\")\n","    # write transformed results\n","    df_test_result.write.parquet(f\"{storage_path}/{RESULT_FILE_NAME}\") \n","else:\n","    print(\"configure storage path to store results\")\n","    df_test_result.show(200,False)    "]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"host":{},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"99940866-4aa4-4893-98d1-1e698b9365f7","default_lakehouse_name":"mdfsparkdemo","default_lakehouse_workspace_id":"0efc371e-81be-4375-8e61-b5a2cadfabce","known_lakehouses":[{"id":"99940866-4aa4-4893-98d1-1e698b9365f7"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
